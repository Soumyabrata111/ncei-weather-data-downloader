{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of hours in the dataset from 2018 to 2022: 43821\n",
      "Total number of missing values in Wind_Speed column: 5833\n",
      "Total number of missing values in Angle column: 5833\n",
      "Length of missing values in Wind_Speed column: 5833\n",
      "Percentage of missing values in Wind_Speed column: 13.31%\n",
      "Percentage of missing values in Angle column: 13.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\soumy\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:5029: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Disc loss: 1.5201137065887451, Gen loss: 2.302030324935913\n",
      "Epoch 100, Disc loss: 1.1287139654159546, Gen loss: 1.1139798164367676\n",
      "Epoch 200, Disc loss: 1.3680866956710815, Gen loss: 0.7263244986534119\n",
      "\n",
      "Statistical Measures:\n",
      "Mean (Real): [ 36.0455407  213.67984627]\n",
      "Mean (Generated): [ 37.2517082 216.6869292]\n",
      "Standard Deviation (Real): [15.45856841 93.19291159]\n",
      "Standard Deviation (Generated): [18.65082669 85.00387705]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "K.set_epsilon(1e-8)\n",
    "\n",
    "# Read and preprocess data\n",
    "df = pd.read_csv('CHHATRAPATI SHIVAJI INTERNATIONAL, IN.csv', usecols=['DATE','REPORT_TYPE', 'WND'])\n",
    "df[['Angle', 'Angle_Measurement_Quality', 'Wind_Obs_Character', 'Wind_Speed', 'Wind_Speed_Quality']] = df['WND'].str.split(\",\", expand=True)\n",
    "\n",
    "df = df.astype({'Angle': float, 'Angle_Measurement_Quality': float, 'Wind_Obs_Character': str, 'Wind_Speed': float, 'Wind_Speed_Quality': float})\n",
    "\n",
    "df = df[(df['Angle'] != 999) & (df['Angle_Measurement_Quality'] == 1) & (df['Wind_Obs_Character'] == 'N') & (df['Wind_Speed'] != 9999) & (df['Wind_Speed_Quality'] == 1) & (df['REPORT_TYPE'] == 'FM-15')]\n",
    "\n",
    "df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "df['Year'] = df['DATE'].dt.year\n",
    "df['Month'] = df['DATE'].dt.month\n",
    "df['Day'] = df['DATE'].dt.day\n",
    "df['Hour'] = df['DATE'].dt.hour\n",
    "df['Minutes'] = df['DATE'].dt.minute\n",
    "df['Seconds'] = df['DATE'].dt.second\n",
    "\n",
    "df = df[['Year', 'Month', 'Day', 'Hour', 'Minutes', 'Seconds','REPORT_TYPE', 'Wind_Speed', 'Angle']]\n",
    "df = df[(df['Minutes'] == 0) & (df['Seconds'] == 0)]\n",
    "df.to_csv('Modified_CHHATRAPATI SHIVAJI INTERNATIONAL, IN.csv', index=False)\n",
    "# Create a DataFrame with hourly timestamps from 2012 to 2022\n",
    "date_range = pd.date_range(start='2018-01-01', end='2022-12-31', freq='H')\n",
    "df1 = pd.DataFrame(date_range, columns=['Date'])\n",
    "df1[['Year', 'Month', 'Day', 'Hour', 'Minutes', 'Seconds']] = df1.apply(lambda x: [x.Date.year, x.Date.month, x.Date.day, x.Date.hour, x.Date.minute, x.Date.second], axis=1, result_type=\"expand\")\n",
    "\n",
    "merged_df = pd.merge(df1, df, on=['Year', 'Month', 'Day', 'Hour', 'Minutes', 'Seconds'], how='outer')\n",
    "merged_df[['REPORT_TYPE', 'Wind_Speed', 'Angle']] = merged_df[['REPORT_TYPE', 'Wind_Speed', 'Angle']].fillna(value=pd.NA)\n",
    "\n",
    "wind_speed_missing_pct = merged_df['Wind_Speed'].isna().mean() * 100\n",
    "angle_missing_pct = merged_df['Angle'].isna().mean() * 100\n",
    "\n",
    "\n",
    "# Print total number of hours in the dataset from 2018 to 2022\n",
    "print(f\"Total number of hours in the dataset from 2018 to 2022: {len(merged_df)}\")\n",
    "# Print the total number of missing values in the Wind_Speed columns\n",
    "print(f\"Total number of missing values in Wind_Speed column: {merged_df['Wind_Speed'].isna().sum()}\")\n",
    "print(f\"Total number of missing values in Angle column: {merged_df['Angle'].isna().sum()}\")\n",
    "\n",
    "# Store the length of missing values in Wind_Speed column\n",
    "wind_speed_missing_len = merged_df['Wind_Speed'].isna().sum()\n",
    "\n",
    "# Print the length of missing values in Wind_Speed column\n",
    "print(f\"Length of missing values in Wind_Speed column: {wind_speed_missing_len}\")\n",
    "\n",
    "print(f\"Percentage of missing values in Wind_Speed column: {wind_speed_missing_pct:.2f}%\")\n",
    "print(f\"Percentage of missing values in Angle column: {angle_missing_pct:.2f}%\")\n",
    "\n",
    "merged_df.to_csv('Merged_Modified_CHHATRAPATI SHIVAJI INTERNATIONAL, IN.csv', index=False)\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# Define the GAN architecture\n",
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(64, input_dim=100, kernel_initializer='he_normal'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dense(64, kernel_initializer='he_normal'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dense(2, activation='linear'))\n",
    "    return model\n",
    "\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(64, input_dim=2, kernel_initializer='he_normal'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dense(64, kernel_initializer='he_normal'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Define the loss functions\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "# Define the optimizer\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-5)\n",
    "\n",
    "# Define the training loop\n",
    "@tf.function\n",
    "def train_step(generator_model, discriminator_model, real_data):\n",
    "    available_data = real_data[:, :2]\n",
    "    missing_indices = tf.math.reduce_any(tf.math.is_nan(available_data), axis=1)\n",
    "    \n",
    "    noise = tf.random.normal([len(available_data), 100])\n",
    "    generated_data = generator_model(noise)\n",
    "    generated_data = tf.cast(generated_data, dtype=tf.float32)  # Cast generated_data to float32\n",
    "    available_data = tf.cast(available_data, dtype=tf.float32)  # Cast available_data to float32\n",
    "    available_data = tf.where(tf.math.is_nan(available_data), generated_data, available_data)\n",
    "\n",
    "    fake_data = tf.concat([available_data[~missing_indices], generated_data], axis=0)  # Create fake_data\n",
    "\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        real_output = discriminator_model(available_data, training=True)\n",
    "        fake_output = discriminator_model(generated_data, training=True)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator_model.trainable_variables)\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator_model.trainable_variables))\n",
    "\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        generated_data = generator_model(noise, training=True)\n",
    "        fake_output = discriminator_model(generated_data, training=True)\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator_model.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator_model.trainable_variables))\n",
    "\n",
    "    return disc_loss, gen_loss, missing_indices\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Train the GAN\n",
    "generator_model = make_generator_model()\n",
    "discriminator_model = make_discriminator_model()\n",
    "\n",
    "available_data = merged_df[['Wind_Speed', 'Angle']].values\n",
    "missing_indices = np.isnan(available_data).any(axis=1)\n",
    "\n",
    "scaler.fit(available_data[~missing_indices])\n",
    "available_data = scaler.transform(available_data)\n",
    "\n",
    "real_data = available_data[~missing_indices]\n",
    "fake_data = available_data[missing_indices]\n",
    "\n",
    "# for epoch in range(1000):\n",
    "for epoch in range(201):\n",
    "    for i in range(100):\n",
    "        real_data_batch = real_data[np.random.choice(len(real_data), size=128, replace=False)]\n",
    "        disc_loss, gen_loss, _ = train_step(generator_model, discriminator_model, real_data_batch)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Disc loss: {disc_loss.numpy()}, Gen loss: {gen_loss.numpy()}')\n",
    "\n",
    "# Generate predictions for the missing data\n",
    "noise = tf.random.normal([len(fake_data), 100])\n",
    "generated_data = generator_model(noise).numpy()\n",
    "available_data[missing_indices] = generated_data\n",
    "available_data = scaler.inverse_transform(available_data)\n",
    "\n",
    "# Update the merged_df DataFrame\n",
    "merged_df.loc[:, 'Wind_Speed'] = available_data[:, 0]\n",
    "merged_df.loc[:, 'Angle'] = available_data[:, 1]\n",
    "# Define datetime for saving the updated DataFrame\n",
    "from datetime import datetime\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "real_data = df[['Wind_Speed', 'Angle']].values\n",
    "generated_data = merged_df.loc[missing_indices, ['Wind_Speed', 'Angle']].values\n",
    "\n",
    "# Statistical measures\n",
    "print(\"\\nStatistical Measures:\")\n",
    "real_mean = np.nanmean(real_data, axis=0)\n",
    "generated_mean = np.mean(generated_data, axis=0)\n",
    "print(f\"Mean (Real): {real_mean}\")\n",
    "print(f\"Mean (Generated): {generated_mean}\")\n",
    "\n",
    "real_std = np.nanstd(real_data, axis=0)\n",
    "generated_std = np.std(generated_data, axis=0)\n",
    "print(f\"Standard Deviation (Real): {real_std}\")\n",
    "print(f\"Standard Deviation (Generated): {generated_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution Similarity:\n",
      "Wind_Speed Kolmogorov-Smirnov Test: KS Statistic = 0.12114140242722571, P-value = 3.948681462813591e-65\n",
      "Angle Kolmogorov-Smirnov Test: KS Statistic = 0.1752396079998626, P-value = 2.7064205340243533e-136\n",
      "\n",
      "Cross-correlation:\n",
      "Cross-correlation (Wind_Speed): 8709725.803186662\n",
      "Cross-correlation (Angle): 290012038.40492976\n"
     ]
    }
   ],
   "source": [
    "# Distribution similarity\n",
    "print(\"\\nDistribution Similarity:\")\n",
    "ks_stat, ks_pvalue = ks_2samp(real_data[:, 0], generated_data[:, 0])\n",
    "print(f\"Wind_Speed Kolmogorov-Smirnov Test: KS Statistic = {ks_stat}, P-value = {ks_pvalue}\")\n",
    "\n",
    "ks_stat, ks_pvalue = ks_2samp(real_data[:, 1], generated_data[:, 1])\n",
    "print(f\"Angle Kolmogorov-Smirnov Test: KS Statistic = {ks_stat}, P-value = {ks_pvalue}\")\n",
    "\n",
    "# Cross-correlation\n",
    "print(\"\\nCross-correlation:\")\n",
    "cross_corr_wind_speed = np.correlate(real_data[:, 0], generated_data[:, 0], mode='valid')[0]\n",
    "cross_corr_angle = np.correlate(real_data[:, 1], generated_data[:, 1], mode='valid')[0]\n",
    "print(f\"Cross-correlation (Wind_Speed): {cross_corr_wind_speed}\")\n",
    "print(f\"Cross-correlation (Angle): {cross_corr_angle}\")\n",
    "\n",
    "# Save the updated DataFrame to a CSV file\n",
    "merged_df.to_csv(f'GAN_Merged_CHHATRAPATI_SHIVAJI_INTERNATIONAL_IN_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "from scipy.signal import correlate\n",
    "\n",
    "def objective_function(generated_data, real_data):\n",
    "    wind_speed_gen = generated_data[:, 0]\n",
    "    angle_gen = generated_data[:, 1]\n",
    "    wind_speed_real = real_data[:, 0]\n",
    "    angle_real = real_data[:, 1]\n",
    "\n",
    "    ks_stat_wind_speed, p_value_wind_speed = ks_2samp(wind_speed_gen, wind_speed_real)\n",
    "    ks_stat_angle, p_value_angle = ks_2samp(angle_gen, angle_real)\n",
    "\n",
    "    cross_correlation_wind_speed = np.max(correlate(wind_speed_gen, wind_speed_real))\n",
    "    cross_correlation_angle = np.max(correlate(angle_gen, angle_real))\n",
    "\n",
    "    mean_diff_wind_speed = np.abs(np.mean(wind_speed_gen) - np.mean(wind_speed_real))\n",
    "    mean_diff_angle = np.abs(np.mean(angle_gen) - np.mean(angle_real))\n",
    "\n",
    "    std_diff_wind_speed = np.abs(np.std(wind_speed_gen) - np.std(wind_speed_real))\n",
    "    std_diff_angle = np.abs(np.std(angle_gen) - np.std(angle_real))\n",
    "    # Assign crowding_dist attribute for each individual\n",
    "    generated_data.fitness.crowding_dist = 0\n",
    "\n",
    "    return (-ks_stat_wind_speed - ks_stat_angle,\n",
    "            p_value_wind_speed + p_value_angle,\n",
    "            cross_correlation_wind_speed + cross_correlation_angle,\n",
    "            -mean_diff_wind_speed - mean_diff_angle,\n",
    "            -std_diff_wind_speed - std_diff_angle)\n",
    "\n",
    "    # return (-ks_stat_wind_speed - ks_stat_angle,\n",
    "    #         p_value_wind_speed + p_value_angle,\n",
    "    #         cross_correlation_wind_speed + cross_correlation_angle,\n",
    "    #         -mean_diff_wind_speed - mean_diff_angle,\n",
    "    #         -std_diff_wind_speed - std_diff_angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from deap import base, creator, tools\n",
    "\n",
    "# Create fitness and individual classes\n",
    "creator.create(\"FitnessMulti\", base.Fitness, weights=(-1.0, 1.0, 1.0, -1.0, -1.0))\n",
    "creator.create(\"Individual\", np.ndarray, fitness=creator.FitnessMulti)\n",
    "\n",
    "# Initialize the GA toolbox\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# Define a function to generate a random individual\n",
    "def random_individual():\n",
    "    return np.random.normal(loc=available_data[missing_indices], scale=0.1)\n",
    "\n",
    "toolbox.register(\"individual\", tools.initIterate, creator.Individual, random_individual)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Define the evaluation, crossover, mutation, and selection functions\n",
    "toolbox.register(\"evaluate\", objective_function)\n",
    "toolbox.register(\"mate\", tools.cxUniform, indpb=0.5)\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.1, indpb=0.1)\n",
    "toolbox.register(\"select\", tools.selNSGA2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ga(population_size, generations, real_data):\n",
    "    # Initialize the population\n",
    "    pop = toolbox.population(n=population_size)\n",
    "\n",
    "    # Evaluate the individuals\n",
    "    fitnesses = list(map(toolbox.evaluate, pop, [real_data] * len(pop)))\n",
    "    for ind, fit in zip(pop, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    for gen in range(generations):\n",
    "        offspring = tools.selTournamentDCD(pop, len(pop))\n",
    "        offspring = list(offspring)\n",
    "\n",
    "        # Apply crossover and mutation\n",
    "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "            toolbox.mate(child1, child2)\n",
    "\n",
    "        for mutant in offspring:\n",
    "            toolbox.mutate(mutant)\n",
    "\n",
    "        # Evaluate the offspring\n",
    "        fitnesses = list(map(toolbox.evaluate, offspring, [real_data] * len(offspring)))\n",
    "        for ind, fit in zip(offspring, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # Replace the old population with the offspring\n",
    "        pop[:] = offspring\n",
    "\n",
    "    return pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set population size and generations\n",
    "# population_size = 100\n",
    "# generations = 50\n",
    "\n",
    "# # Run the GA and get the optimized generated data\n",
    "# optimized_pop = run_ga(population_size, generations, real_data)\n",
    "\n",
    "# # Get the best individual based on the first objective (minimize KS statistic)\n",
    "# best_ind = tools.selBest(optimized_pop, k=1, fit_attr='fitness')[0]\n",
    "\n",
    "# # Update the merged_df DataFrame with the optimized generated data\n",
    "# merged_df.loc[missing_indices, ['Wind_Speed', 'Angle']] = best_ind\n",
    "\n",
    "# # Save the updated DataFrame with the optimized generated data to a CSV file with current timestamp\n",
    "\n",
    "# merged_df.to_csv(f'Merged_Modified_Optimized_CHHATRAPATI_SHIVAJI_INTERNATIONAL_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv', index=False)\n",
    "# Set population size and generations\n",
    "population_size = 100\n",
    "generations = 50\n",
    "\n",
    "# Run the GA and get the optimized generated data\n",
    "optimized_pop = run_ga(population_size, generations, real_data)\n",
    "\n",
    "# Get the best individual based on the first objective (minimize KS statistic)\n",
    "best_ind = tools.selBest(optimized_pop, k=1, fit_attr='fitness')[0]\n",
    "\n",
    "# Ensure no negative values for wind speed and angle\n",
    "best_ind[:, 0] = np.abs(best_ind[:, 0])\n",
    "best_ind[:, 1] = np.abs(best_ind[:, 1])\n",
    "\n",
    "# Update the merged_df DataFrame with the optimized generated data\n",
    "merged_df.loc[missing_indices, ['Wind_Speed', 'Angle']] = best_ind\n",
    "\n",
    "# Save the updated DataFrame with the optimized generated data to a CSV file with current timestamp\n",
    "merged_df.to_csv(f'Merged_Modified_Optimized_CHHATRAPATI_SHIVAJI_INTERNATIONAL_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Statistical Measures:\n",
      "Mean:\n",
      "Wind_Speed: 36.244749107401084\n",
      "Angle: 214.10643247379167\n",
      "\n",
      "Standard Deviation:\n",
      "Wind_Speed: 15.838195021235375\n",
      "Angle: 92.091832455794\n",
      "\n",
      "Distribution Similarity:\n",
      "Wind_Speed Kolmogorov-Smirnov Test: KS Statistic = 0.12131284079513244, P-value = 2.585988714053331e-65\n",
      "Angle Kolmogorov-Smirnov Test: KS Statistic = 0.17541104636776939, P-value = 1.460848492895949e-136\n",
      "\n",
      "Cross-correlation:\n",
      "Cross-correlation (Wind_Speed): 8776360.740917321\n",
      "Cross-correlation (Angle): 290283777.85183764\n"
     ]
    }
   ],
   "source": [
    "# Calculate the new Statistical measures Distribution similarity and Cross-correlation\n",
    "print(\"Optimized Statistical Measures:\")\n",
    "print(\"Mean:\")\n",
    "print(f\"Wind_Speed: {merged_df['Wind_Speed'].mean()}\")\n",
    "print(f\"Angle: {merged_df['Angle'].mean()}\")\n",
    "print(\"\\nStandard Deviation:\")\n",
    "print(f\"Wind_Speed: {merged_df['Wind_Speed'].std()}\")\n",
    "print(f\"Angle: {merged_df['Angle'].std()}\")\n",
    "print(\"\\nDistribution Similarity:\")\n",
    "ks_stat, ks_pvalue = ks_2samp(real_data[:, 0], best_ind[:, 0])\n",
    "print(f\"Wind_Speed Kolmogorov-Smirnov Test: KS Statistic = {ks_stat}, P-value = {ks_pvalue}\")\n",
    "ks_stat, ks_pvalue = ks_2samp(real_data[:, 1], best_ind[:, 1])\n",
    "print(f\"Angle Kolmogorov-Smirnov Test: KS Statistic = {ks_stat}, P-value = {ks_pvalue}\")\n",
    "print(\"\\nCross-correlation:\")\n",
    "cross_corr_wind_speed = np.correlate(real_data[:, 0], best_ind[:, 0], mode='valid')[0]\n",
    "cross_corr_angle = np.correlate(real_data[:, 1], best_ind[:, 1], mode='valid')[0]\n",
    "print(f\"Cross-correlation (Wind_Speed): {cross_corr_wind_speed}\")\n",
    "print(f\"Cross-correlation (Angle): {cross_corr_angle}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Calculate the new Statistical measures Distribution similarity and Cross-correlation\n",
    "optimized_measures = {\n",
    "    \"Mean\": {\n",
    "        \"Wind_Speed\": merged_df['Wind_Speed'].mean(),\n",
    "        \"Angle\": merged_df['Angle'].mean()\n",
    "    },\n",
    "    \"Standard Deviation\": {\n",
    "        \"Wind_Speed\": merged_df['Wind_Speed'].std(),\n",
    "        \"Angle\": merged_df['Angle'].std()\n",
    "    },\n",
    "    \"Distribution Similarity\": {\n",
    "        \"Wind_Speed_KS_Statistic\": ks_2samp(real_data[:, 0], best_ind[:, 0])[0],\n",
    "        \"Wind_Speed_P-value\": ks_2samp(real_data[:, 0], best_ind[:, 0])[1],\n",
    "        \"Angle_KS_Statistic\": ks_2samp(real_data[:, 1], best_ind[:, 1])[0],\n",
    "        \"Angle_P-value\": ks_2samp(real_data[:, 1], best_ind[:, 1])[1]\n",
    "    },\n",
    "    \"Cross-correlation\": {\n",
    "        \"Wind_Speed\": np.correlate(real_data[:, 0], best_ind[:, 0], mode='valid')[0],\n",
    "        \"Angle\": np.correlate(real_data[:, 1], best_ind[:, 1], mode='valid')[0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Store the statistical measures in a CSV file\n",
    "with open(f'Optimized_Statistical_Measures_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Category', 'Metric', 'Value'])\n",
    "\n",
    "    for category, metrics in optimized_measures.items():\n",
    "        for metric, value in metrics.items():\n",
    "            writer.writerow([category, metric, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Calculate the actual Statistical measures\n",
    "actual_measures = {\n",
    "    \"Mean\": {\n",
    "        \"Wind_Speed\": real_data[:, 0].mean(),\n",
    "        \"Angle\": real_data[:, 1].mean()\n",
    "    },\n",
    "    \"Standard Deviation\": {\n",
    "        \"Wind_Speed\": real_data[:, 0].std(),\n",
    "        \"Angle\": real_data[:, 1].std()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Calculate the new Statistical measures Distribution similarity and Cross-correlation\n",
    "optimized_measures = {\n",
    "    \"Mean\": {\n",
    "        \"Wind_Speed\": merged_df['Wind_Speed'].mean(),\n",
    "        \"Angle\": merged_df['Angle'].mean()\n",
    "    },\n",
    "    \"Standard Deviation\": {\n",
    "        \"Wind_Speed\": merged_df['Wind_Speed'].std(),\n",
    "        \"Angle\": merged_df['Angle'].std()\n",
    "    },\n",
    "    \"Distribution Similarity\": {\n",
    "        \"Wind_Speed_KS_Statistic\": ks_2samp(real_data[:, 0], best_ind[:, 0])[0],\n",
    "        \"Wind_Speed_P-value\": ks_2samp(real_data[:, 0], best_ind[:, 0])[1],\n",
    "        \"Angle_KS_Statistic\": ks_2samp(real_data[:, 1], best_ind[:, 1])[0],\n",
    "        \"Angle_P-value\": ks_2samp(real_data[:, 1], best_ind[:, 1])[1]\n",
    "    },\n",
    "    \"Cross-correlation\": {\n",
    "        \"Wind_Speed\": np.correlate(real_data[:, 0], best_ind[:, 0], mode='valid')[0],\n",
    "        \"Angle\": np.correlate(real_data[:, 1], best_ind[:, 1], mode='valid')[0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Store the statistical measures in a CSV file\n",
    "with open(f'Optimized_Statistical_Measures_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Category', 'Metric', 'Actual Value', 'Optimized Value'])\n",
    "\n",
    "    for category, metrics in optimized_measures.items():\n",
    "        for metric, optimized_value in metrics.items():\n",
    "            actual_value = actual_measures.get(category, {}).get(metric)\n",
    "            writer.writerow([category, metric, actual_value, optimized_value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
